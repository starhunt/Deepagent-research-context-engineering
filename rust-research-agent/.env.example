# =============================================================================
# AI Research Agent - Environment Configuration
# =============================================================================
# Copy this file to .env and customize for your setup.
# Run: cp .env.example .env
# =============================================================================

# =============================================================================
# OLLAMA CONFIGURATION (Required)
# =============================================================================
# The model to use for research tasks.
# Make sure you've pulled this model: ollama pull <model_name>
# 
# Popular options:
# - deepseek-v3.2       (DeepSeek - great for research)
# - qwen3-coder         (Alibaba - good for technical topics)
# - llama3.2            (Meta - general purpose)
# - mistral             (Mistral AI - fast and capable)
# - gemma2              (Google - efficient)
OLLAMA_MODEL=llama3.2

# Ollama server URL (default: http://localhost:11434)
# Change this if running Ollama on a different host/port
OLLAMA_API_BASE_URL=http://localhost:11434

# =============================================================================
# AGENT CONFIGURATION (Optional)
# =============================================================================
# Temperature controls randomness in responses (0.0 = deterministic, 1.0 = creative)
# Lower values produce more focused, factual responses - better for research
TEMPERATURE=0.7

# Maximum number of search results to analyze per query
MAX_SEARCH_RESULTS=5

# =============================================================================
# LOGGING CONFIGURATION (Optional)
# =============================================================================
# Log level: trace, debug, info, warn, error
# Use 'debug' during development, 'info' in production
RUST_LOG=info

# =============================================================================
# GETTING STARTED
# =============================================================================
# 1. Install Ollama: https://ollama.ai
# 2. Pull a model: ollama pull llama3.2
# 3. Start Ollama: ollama serve
# 4. Copy this file: cp .env.example .env
# 5. Run the agent: cargo run -- "Your research topic here"
# =============================================================================
