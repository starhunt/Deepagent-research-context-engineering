# P0 êµ¬í˜„ ê³„íš: í”„ë¡œë•ì…˜ ëŒ€ì²´ë¥¼ ìœ„í•œ í•„ìˆ˜ ì‘ì—…

> **ëª©í‘œ**: Rust rig-deepagentsê°€ Python LangChain DeepAgentë¥¼ í”„ë¡œë•ì…˜ì—ì„œ 100% ëŒ€ì²´í•  ìˆ˜ ìˆë„ë¡ í•µì‹¬ ê¸°ëŠ¥ ì™„ì„±
>
> **ì˜ˆìƒ ê¸°ê°„**: 3-4ì£¼ (Codex ê²€í†  í›„ ì¡°ì •)
> **ìš°ì„ ìˆœìœ„**: P0 (Critical)
> **Reviewed by**: Codex (gpt-5.2-codex) - 2026-01-03

---

## Codex Review Summary

### ë°œê²¬ëœ Critical Issues

| Severity | Issue | Reference |
|----------|-------|-----------|
| ğŸ”´ Critical | `resume()`ê°€ í˜„ì¬ ëŸ°íƒ€ì„ì—ì„œ ë™ì‘ ë¶ˆê°€ - `run_inner`ê°€ í•­ìƒ `superstep = 0`ë¶€í„° ì‹œì‘ | `runtime.rs:167,169` |
| ğŸŸ  High | Checkpointê°€ `WorkflowMessage` í•˜ë“œì½”ë”©, generic `M` ì§€ì› ë¶ˆê°€ | `runtime.rs:52`, `mod.rs:98` |
| ğŸŸ  High | `retry_counts` ë¯¸ì²´í¬í¬ì¸íŒ… â†’ resume ì‹œ ì¬ì‹œë„ ì˜ë¯¸ ë³€ê²½ | `runtime.rs:56,354` |
| ğŸŸ  High | Rig API ë¶ˆì¼ì¹˜: `stream_prompt`ê°€ `MultiTurnStreamItem` ë°˜í™˜ | rig-core 0.27 API |
| ğŸŸ¡ Medium | Tool call íŒŒì‹±ì´ pure JSONë§Œ ì²˜ë¦¬, í˜¼í•© ì‘ë‹µ ëˆ„ë½ ê°€ëŠ¥ | `rig_agent_adapter.rs:215` |

---

## Executive Summary (ìˆ˜ì •ë¨)

| Task | í˜„ì¬ ìƒíƒœ | ëª©í‘œ | ì˜ˆìƒ ê³µìˆ˜ |
|------|----------|------|----------|
| **P0-1: Checkpointing í†µí•©** | êµ¬í˜„ë˜ì—ˆìœ¼ë‚˜ ì—°ê²° ì•ˆë¨ | Runtimeì— wire + resume API | 2ì£¼ |
| **P0-2: RigAgentAdapter ê°•í™”** | ê¸°ë³¸ ê¸°ëŠ¥ë§Œ | Full message + chat API | 1.5ì£¼ |
| **P0-3: Streaming (Optional)** | Stub ìƒíƒœ | MultiTurnStreamItem ë³€í™˜ | 1ì£¼ (defer ê°€ëŠ¥) |

---

## P0-1: Checkpointingì„ PregelRuntimeì— í†µí•©

### í˜„ì¬ ìƒíƒœ ë¶„ì„

**âœ… ì´ë¯¸ êµ¬í˜„ë¨:**
```rust
// src/pregel/checkpoint/mod.rs
pub struct Checkpoint<S> {
    pub workflow_id: String,
    pub superstep: usize,
    pub state: S,
    pub vertex_states: HashMap<VertexId, VertexState>,
    pub pending_messages: HashMap<VertexId, Vec<WorkflowMessage>>,
}

pub trait Checkpointer<S: WorkflowState> {
    async fn save(&self, checkpoint: &Checkpoint<S>) -> Result<(), PregelError>;
    async fn load(&self, superstep: usize) -> Result<Option<Checkpoint<S>>, PregelError>;
    async fn latest(&self) -> Result<Option<Checkpoint<S>>, PregelError>;
}
```

**âœ… ì´ë¯¸ êµ¬í˜„ë¨:**
```rust
// src/pregel/config.rs
impl PregelConfig {
    pub fn should_checkpoint(&self, superstep: usize) -> bool {
        self.checkpointing_enabled() && superstep > 0 && superstep % self.checkpoint_interval == 0
    }
}
```

**âŒ ëˆ„ë½ëœ ë¶€ë¶„:**
- `PregelRuntime`ì´ `Checkpointer`ë¥¼ ë³´ìœ í•˜ì§€ ì•ŠìŒ
- `run()` ë£¨í”„ì—ì„œ `should_checkpoint()` í™•ì¸ í›„ ì €ì¥ ë¡œì§ ì—†ìŒ
- ì²´í¬í¬ì¸íŠ¸ì—ì„œ resumeí•˜ëŠ” ê¸°ëŠ¥ ì—†ìŒ

### êµ¬í˜„ ê³„íš

#### Task 1.1: PregelRuntimeì— Checkpointer ì¶”ê°€ (3ì¼)

> âš ï¸ **Codex í”¼ë“œë°± ë°˜ì˜**: Checkpointë¥¼ `WorkflowMessage` ì „ìš©ìœ¼ë¡œ íŠ¹ìˆ˜í™”í•˜ê³ ,
> `run_from_checkpoint()` API ì¶”ê°€ í•„ìš”

**íŒŒì¼**: `src/pregel/runtime.rs`

**Option A (ê¶Œì¥): WorkflowMessage ì „ìš© impl block**

```rust
// Generic runtimeì€ ë³€ê²½ ì—†ìŒ
pub struct PregelRuntime<S, M> { ... }

// WorkflowMessage ì „ìš© ì²´í¬í¬ì¸íŒ… êµ¬í˜„
impl<S> PregelRuntime<S, WorkflowMessage>
where
    S: WorkflowState,
{
    /// Attach a checkpointer for state persistence
    pub fn with_checkpointer(
        mut self,
        checkpointer: Arc<dyn Checkpointer<S> + Send + Sync>,
        workflow_id: impl Into<String>,
    ) -> Self {
        self.checkpointer = Some(checkpointer);
        self.workflow_id = workflow_id.into();
        self
    }

    /// Run workflow from a checkpoint (for resume)
    ///
    /// # Critical Fix (Codex Review)
    /// ê¸°ì¡´ run()ì€ superstep=0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ, ì²´í¬í¬ì¸íŠ¸ì—ì„œ
    /// ì¬ê°œí•˜ë ¤ë©´ ë³„ë„ì˜ ì§„ì…ì  í•„ìš”
    pub async fn run_from_checkpoint(
        &mut self,
        checkpoint: Checkpoint<S>
    ) -> Result<WorkflowResult<S>, PregelError> {
        // Restore state from checkpoint
        self.restore_from_checkpoint(&checkpoint)?;

        // Continue from checkpoint superstep
        self.run_inner_from(checkpoint.state, checkpoint.superstep).await
    }

    /// Resume from the latest checkpoint
    pub async fn resume(&mut self) -> Result<Option<WorkflowResult<S>>, PregelError> {
        if let Some(checkpointer) = &self.checkpointer {
            if let Some(checkpoint) = checkpointer.latest().await? {
                let result = self.run_from_checkpoint(checkpoint).await?;
                return Ok(Some(result));
            }
        }
        Ok(None)
    }

    /// Internal: Restore vertex states, message queues, and retry counts
    fn restore_from_checkpoint(&mut self, checkpoint: &Checkpoint<S>) -> Result<(), PregelError> {
        // Validate workflow_id matches
        if checkpoint.workflow_id != self.workflow_id {
            return Err(PregelError::CheckpointMismatch {
                expected: self.workflow_id.clone(),
                found: checkpoint.workflow_id.clone(),
            });
        }

        // Restore vertex states
        self.vertex_states = checkpoint.vertex_states.clone();

        // Restore pending messages
        for (vid, messages) in &checkpoint.pending_messages {
            if let Some(queue) = self.message_queues.get_mut(vid) {
                *queue = messages.clone();
            }
        }

        // NEW (Codex í”¼ë“œë°±): Restore retry counts from metadata
        if let Some(retry_json) = checkpoint.metadata.get("retry_counts") {
            if let Ok(counts) = serde_json::from_str(retry_json) {
                self.retry_counts = counts;
            }
        }

        Ok(())
    }

    /// Internal: Create checkpoint including retry_counts
    fn create_checkpoint(&self, superstep: usize, state: &S) -> Checkpoint<S> {
        let mut checkpoint = Checkpoint::new(
            &self.workflow_id,
            superstep,
            state.clone(),
            self.vertex_states.clone(),
            self.message_queues.iter()
                .map(|(k, v)| (k.clone(), v.clone()))
                .collect(),
        );

        // NEW (Codex í”¼ë“œë°±): Include retry_counts in metadata
        if let Ok(retry_json) = serde_json::to_string(&self.retry_counts) {
            checkpoint = checkpoint.with_metadata("retry_counts", retry_json);
        }

        checkpoint
    }
}
```

**ìƒˆë¡œìš´ run_inner_from() ë©”ì„œë“œ (superstep ì‹œì‘ì  ì§€ì •)**:

```rust
async fn run_inner_from(
    &mut self,
    initial_state: S,
    start_superstep: usize
) -> Result<WorkflowResult<S>, PregelError> {
    let mut state = initial_state;
    let mut superstep = start_superstep;

    loop {
        // Check max supersteps (adjusted for resume)
        if superstep >= self.config.max_supersteps {
            return Err(PregelError::MaxSuperstepsExceeded(superstep));
        }

        // ... existing loop logic ...

        superstep += 1;

        // Checkpoint if interval reached
        if self.config.should_checkpoint(superstep) {
            self.save_checkpoint(superstep, &state).await?;
        }
    }
}
```

#### Task 1.2: run() ë£¨í”„ì— Checkpoint ì €ì¥ ë¡œì§ ì¶”ê°€ (1ì¼)

**íŒŒì¼**: `src/pregel/runtime.rs`

```rust
pub async fn run(&mut self, initial_state: S) -> Result<WorkflowResult<S>, PregelError> {
    let mut state = initial_state;
    let mut superstep = 0;

    // ... existing timeout wrapper ...

    loop {
        // Check termination
        if self.is_terminated(&state) {
            break;
        }

        // Execute superstep
        let updates = self.execute_superstep(&state).await?;
        state = state.apply_updates(updates);
        superstep += 1;

        // NEW: Checkpoint if interval reached
        if self.config.should_checkpoint(superstep) {
            self.save_checkpoint(superstep, &state).await?;
        }

        // Check max supersteps
        if superstep >= self.config.max_supersteps {
            break;
        }
    }

    // ... return result ...
}

async fn save_checkpoint(&self, superstep: usize, state: &S) -> Result<(), PregelError> {
    if let Some(checkpointer) = &self.checkpointer {
        let checkpoint = self.create_checkpoint(superstep, state);
        checkpointer.save(&checkpoint).await?;
        tracing::info!(superstep, "Checkpoint saved");
    }
    Ok(())
}
```

#### Task 1.3: CompiledWorkflowì— Checkpointer ì „ë‹¬ (1ì¼)

**íŒŒì¼**: `src/workflow/compiled.rs`

```rust
impl<S: WorkflowState + Serialize> CompiledWorkflow<S> {
    /// Compile with checkpointer for fault tolerance
    pub fn compile_with_checkpointer(
        graph: BuiltWorkflowGraph<S>,
        config: PregelConfig,
        checkpointer: Arc<dyn Checkpointer<S> + Send + Sync>,
        workflow_id: impl Into<String>,
    ) -> Result<Self, WorkflowCompileError> {
        // ... existing compilation ...
        // Pass checkpointer to runtime
    }

    /// Resume workflow from latest checkpoint
    pub async fn resume(&mut self) -> Result<Option<usize>, PregelError> {
        self.runtime.resume().await
    }
}
```

#### Task 1.4: í…ŒìŠ¤íŠ¸ ì‘ì„± (2ì¼)

**íŒŒì¼**: `tests/integration_checkpointing.rs`

```rust
#[tokio::test]
async fn test_checkpoint_save_during_execution() {
    // Verify checkpoints are created at correct intervals
}

#[tokio::test]
async fn test_resume_from_checkpoint() {
    // Verify workflow can resume from saved state
}

#[tokio::test]
async fn test_checkpoint_with_pending_messages() {
    // Verify messages are preserved across checkpoint/resume
}

#[tokio::test]
async fn test_checkpoint_backend_integration() {
    // Test with File/SQLite backends
}
```

---

## P0-2: RigAgentAdapter ê°•í™”

### í˜„ì¬ ìƒíƒœ ë¶„ì„

**âŒ ë¬¸ì œì :**
```rust
// src/compat/rig_agent_adapter.rs:167-186
fn build_prompt_with_tools(messages: &[Message], tools: &[ToolDefinition]) -> String {
    // ...
    // Find the last user message â† ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ì‚¬ìš©!
    let last_user_msg = messages
        .iter()
        .rfind(|m| m.role == Role::User)
        .map(|m| m.content.clone())
        .unwrap_or_default();
    // ...
}
```

```rust
// src/compat/rig_agent_adapter.rs:122-127
async fn complete(
    &self,
    messages: &[Message],
    tools: &[ToolDefinition],
    _config: Option<&LLMConfig>,  // â† _config ë¬´ì‹œë¨!
) -> Result<LLMResponse, DeepAgentError> {
```

### êµ¬í˜„ ê³„íš

#### Task 2.1: Rig Chat API í†µí•© (3ì¼)

> âš ï¸ **Codex í”¼ë“œë°± ë°˜ì˜**: ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ ëŒ€ì‹  Rigì˜ `Message` íƒ€ì…ì„
> ì‚¬ìš©í•˜ì—¬ `agent.completion(prompt, history)` í˜¸ì¶œ

**íŒŒì¼**: `src/compat/rig_agent_adapter.rs`

**ìƒˆë¡œìš´ ì ‘ê·¼ë²•: Rig Message ë³€í™˜**

```rust
use rig::completion::Message as RigMessage;

/// Convert rig-deepagents messages to Rig's native Message format
fn convert_to_rig_messages(messages: &[Message]) -> (RigMessage, Vec<RigMessage>) {
    let mut history = Vec::new();

    for msg in messages.iter().take(messages.len().saturating_sub(1)) {
        let rig_msg = match msg.role {
            Role::System => RigMessage::system(&msg.content),
            Role::User => RigMessage::user(&msg.content),
            Role::Assistant => {
                // Include tool calls in assistant message if present
                if let Some(tool_calls) = &msg.tool_calls {
                    let calls_str = tool_calls.iter()
                        .map(|tc| format!("[{}({})]", tc.name, tc.arguments))
                        .collect::<Vec<_>>()
                        .join(" ");
                    RigMessage::assistant(format!("{}\n{}", msg.content, calls_str))
                } else {
                    RigMessage::assistant(&msg.content)
                }
            },
            Role::Tool => RigMessage::user(format!("[Tool Result]: {}", msg.content)),
        };
        history.push(rig_msg);
    }

    // Last message becomes the prompt
    let prompt = messages.last()
        .map(|m| RigMessage::user(&m.content))
        .unwrap_or_else(|| RigMessage::user(""));

    (prompt, history)
}
```

**ìˆ˜ì •ëœ complete() ë©”ì„œë“œ**:

```rust
#[async_trait]
impl<M> LLMProvider for RigAgentAdapter<M>
where
    M: CompletionModel + Send + Sync + 'static,
{
    async fn complete(
        &self,
        messages: &[Message],
        tools: &[ToolDefinition],
        config: Option<&LLMConfig>,
    ) -> Result<LLMResponse, DeepAgentError> {
        // Convert to Rig message format
        let (prompt, history) = convert_to_rig_messages(messages);

        // Include tool schemas in system context
        let tools_context = if !tools.is_empty() {
            Some(build_tools_section(tools))
        } else {
            None
        };

        // Use Rig's completion API with chat history
        // (Codex í”¼ë“œë°±: Agent::completion requires prompt + history)
        let response = self.agent
            .completion(prompt, history)
            .await
            .map_err(|e| DeepAgentError::LlmError(e.to_string()))?
            .send()
            .await
            .map_err(|e| DeepAgentError::LlmError(e.to_string()))?;

        // Parse response with improved tool call extraction
        let message = parse_response_for_tool_calls(&response.output);

        Ok(LLMResponse::new(message))
    }
}
```

**ê°œì„ ëœ Tool Call íŒŒì‹± (í˜¼í•© ì‘ë‹µ ì²˜ë¦¬)**:

```rust
/// Parse LLM response for potential tool calls
///
/// Codex í”¼ë“œë°±: í˜¼í•© í…ìŠ¤íŠ¸+JSON ì‘ë‹µì—ì„œ tool calls ì¶”ì¶œ
fn parse_response_for_tool_calls(response: &str) -> Message {
    // 1. Try pure JSON first
    if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(response) {
        if let Some(tool_calls_val) = parsed.get("tool_calls") {
            if let Ok(tool_calls) = extract_tool_calls(tool_calls_val) {
                return Message::assistant_with_tool_calls("", tool_calls);
            }
        }
    }

    // 2. NEW: Try to find JSON in mixed text/JSON response
    if let Some(json_start) = response.find("{\"tool_calls\"") {
        if let Some(json_end) = find_matching_brace(response, json_start) {
            let json_part = &response[json_start..=json_end];
            if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(json_part) {
                if let Some(tool_calls_val) = parsed.get("tool_calls") {
                    if let Ok(tool_calls) = extract_tool_calls(tool_calls_val) {
                        let text_part = response[..json_start].trim();
                        return Message::assistant_with_tool_calls(text_part, tool_calls);
                    }
                }
            }
        }
    }

    // 3. Default: treat as normal text response
    Message::assistant(response)
}

/// Find matching closing brace for JSON extraction
fn find_matching_brace(s: &str, start: usize) -> Option<usize> {
    let mut depth = 0;
    for (i, c) in s[start..].char_indices() {
        match c {
            '{' => depth += 1,
            '}' => {
                depth -= 1;
                if depth == 0 {
                    return Some(start + i);
                }
            }
            _ => {}
        }
    }
    None
}
```

#### Task 2.2: LLMConfig ì ìš© (1ì¼)

**íŒŒì¼**: `src/compat/rig_agent_adapter.rs`

```rust
async fn complete(
    &self,
    messages: &[Message],
    tools: &[ToolDefinition],
    config: Option<&LLMConfig>,
) -> Result<LLMResponse, DeepAgentError> {
    let prompt = build_full_prompt(messages, tools);

    // Apply config if provided
    let response = if let Some(cfg) = config {
        // Use Rig's completion request builder with config
        self.agent
            .completion(&prompt)
            .temperature(cfg.temperature.unwrap_or(0.0) as f32)
            .max_tokens(cfg.max_tokens.unwrap_or(4096) as u32)
            .send()
            .await
    } else {
        self.agent.prompt(&prompt).await
    };

    // ... rest of processing ...
}
```

**ì°¸ê³ **: Rigì˜ `Agent.completion()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ temperature, max_tokens ë“± ì„¤ì • ì ìš©

#### Task 2.3: Streaming êµ¬í˜„ (3ì¼) - Optional/Deferrable

> âš ï¸ **Codex í”¼ë“œë°±**: Rigì˜ `stream_prompt`ëŠ” `MultiTurnStreamItem` ë°˜í™˜,
> `Result<String, rig::Error>` ì•„ë‹˜. ë³µì¡ì„±ìœ¼ë¡œ ì¸í•´ defer ê³ ë ¤

**íŒŒì¼**: `src/compat/rig_agent_adapter.rs`

**ìˆ˜ì •ëœ Streaming êµ¬í˜„ (Rig API ì¤€ìˆ˜)**:

```rust
use rig::agent::prompt_request::streaming::{
    StreamingResult, MultiTurnStreamItem, StreamedAssistantContent
};

async fn stream(
    &self,
    messages: &[Message],
    tools: &[ToolDefinition],
    config: Option<&LLMConfig>,
) -> Result<LLMResponseStream, DeepAgentError> {
    let (prompt, history) = convert_to_rig_messages(messages);

    // Use Rig's streaming API (returns MultiTurnStreamItem stream)
    let rig_stream = self.agent
        .stream_chat(prompt, history)
        .await
        .map_err(|e| DeepAgentError::LlmError(e.to_string()))?;

    // Convert Rig stream to our LLMResponseStream
    Ok(LLMResponseStream::from_rig_stream(rig_stream))
}
```

**íŒŒì¼**: `src/llm/provider.rs` - LLMResponseStream í™•ì¥

```rust
use futures::StreamExt;

impl LLMResponseStream {
    /// Create from Rig's native MultiTurnStreamItem stream
    ///
    /// Codex í”¼ë“œë°±: MultiTurnStreamItem variants:
    /// - StreamAssistantItem(StreamedAssistantContent<R>)
    /// - StreamUserItem(StreamedUserContent)
    /// - Final(usage)
    pub fn from_rig_stream<R>(
        stream: StreamingResult<R>
    ) -> Self
    where
        R: Send + 'static,
    {
        let converted = stream.filter_map(|item| async {
            match item {
                Ok(MultiTurnStreamItem::StreamAssistantItem(content)) => {
                    // Extract text delta from StreamedAssistantContent
                    match content {
                        StreamedAssistantContent::Text(text) => {
                            Some(MessageChunk::Content(text.text))
                        }
                        StreamedAssistantContent::ToolCall(tc) => {
                            Some(MessageChunk::ToolCall {
                                id: tc.id,
                                name: tc.name,
                                arguments_delta: tc.arguments,
                            })
                        }
                        _ => None,
                    }
                }
                Ok(MultiTurnStreamItem::Final(usage)) => {
                    Some(MessageChunk::Usage {
                        input_tokens: usage.input_tokens,
                        output_tokens: usage.output_tokens,
                    })
                }
                _ => None,
            }
        });

        LLMResponseStream::new(Box::pin(converted))
    }
}
```

**ëŒ€ì•ˆ: Streaming ì—°ê¸° (ê¶Œì¥)**

Streaming ë³µì¡ì„±ìœ¼ë¡œ ì¸í•´ P0ì—ì„œ ì œì™¸í•˜ê³  P1ìœ¼ë¡œ ì—°ê¸°:

```rust
async fn stream(...) -> Result<LLMResponseStream, DeepAgentError> {
    // Fallback to complete (existing behavior)
    tracing::warn!("Streaming not yet implemented, falling back to complete");
    let response = self.complete(messages, tools, config).await?;
    Ok(LLMResponseStream::from_complete(response))
}
```

#### Task 2.4: í…ŒìŠ¤íŠ¸ ê°•í™” (1ì¼)

**íŒŒì¼**: `src/compat/rig_agent_adapter.rs` (test module)

```rust
#[test]
fn test_build_full_prompt_conversation_history() {
    let messages = vec![
        Message::system("You are helpful"),
        Message::user("Hello"),
        Message::assistant("Hi there!"),
        Message::user("What is 2+2?"),
    ];

    let prompt = build_full_prompt(&messages, &[]);

    assert!(prompt.contains("System"));
    assert!(prompt.contains("You are helpful"));
    assert!(prompt.contains("Hello"));
    assert!(prompt.contains("Hi there!"));
    assert!(prompt.contains("What is 2+2?"));
}

#[test]
fn test_build_full_prompt_with_tool_calls() {
    let messages = vec![
        Message::user("Search for Rust"),
        Message::assistant_with_tool_calls("", vec![
            ToolCall { id: "1".into(), name: "search".into(), arguments: json!({"q": "Rust"}) }
        ]),
        Message::tool("1", "Results: ..."),
    ];

    let prompt = build_full_prompt(&messages, &[]);

    assert!(prompt.contains("Tool Call: search"));
    assert!(prompt.contains("Tool Result"));
}
```

---

## ì‹¤í–‰ ìˆœì„œ (Codex í”¼ë“œë°± ë°˜ì˜)

```
Week 1:
â”œâ”€â”€ Day 1-3: Task 1.1 - PregelRuntimeì— Checkpointer ì¶”ê°€
â”‚            (run_from_checkpoint, retry_counts í¬í•¨)
â”œâ”€â”€ Day 4: Task 1.2 - run_inner_from() + ì €ì¥ ë¡œì§
â””â”€â”€ Day 5: Task 1.3 - CompiledWorkflow ì—°ë™

Week 2:
â”œâ”€â”€ Day 1-3: Task 2.1 - Rig Chat API í†µí•©
â”‚            (Message ë³€í™˜, completion(prompt, history))
â”œâ”€â”€ Day 4: Task 2.2 - LLMConfig ì ìš©
â””â”€â”€ Day 5: Task 1.4 - Checkpointing í†µí•© í…ŒìŠ¤íŠ¸

Week 3:
â”œâ”€â”€ Day 1-2: Task 2.4 - ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸ ê°•í™”
â”œâ”€â”€ Day 3: EdgeDriven vs MessageBased resume í…ŒìŠ¤íŠ¸
â”œâ”€â”€ Day 4: ë²„ê·¸ ìˆ˜ì • ë° ì—£ì§€ ì¼€ì´ìŠ¤ ì²˜ë¦¬
â””â”€â”€ Day 5: ë¬¸ì„œ ì—…ë°ì´íŠ¸ ë° ìµœì¢… ê²€ì¦

Week 4 (Optional):
â”œâ”€â”€ Day 1-3: Task 2.3 - Streaming êµ¬í˜„ (P1ìœ¼ë¡œ ì—°ê¸° ê°€ëŠ¥)
â””â”€â”€ Day 4-5: ìµœì¢… E2E í…ŒìŠ¤íŠ¸
```

### Scope ì¡°ì • ì˜µì…˜

| Option | ë²”ìœ„ | ê¸°ê°„ | ê¶Œì¥ |
|--------|------|------|------|
| **A: Full** | Checkpointing + Adapter + Streaming | 4ì£¼ | |
| **B: Core (ê¶Œì¥)** | Checkpointing + Adapter (Streaming defer) | 3ì£¼ | âœ… |
| **C: Minimal** | Checkpointing only | 2ì£¼ | |

---

## ê²€ì¦ ê¸°ì¤€

### P0-1 ì™„ë£Œ ì¡°ê±´
- [ ] `cargo test checkpoint` - ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ê´€ë ¨ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì§€ì •ëœ intervalì— ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í™•ì¸
- [ ] ì²´í¬í¬ì¸íŠ¸ì—ì„œ resume í›„ ì •ìƒ ì‹¤í–‰ í™•ì¸
- [ ] File/SQLite ë°±ì—”ë“œë¡œ E2E í…ŒìŠ¤íŠ¸ í†µê³¼

### P0-2 ì™„ë£Œ ì¡°ê±´
- [ ] `cargo test rig_agent_adapter` - ëª¨ë“  ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ë©€í‹°í„´ ëŒ€í™”ì—ì„œ ì „ì²´ íˆìŠ¤í† ë¦¬ í¬í•¨ í™•ì¸
- [ ] LLMConfig (temperature, max_tokens) ì ìš© í™•ì¸
- [ ] Streaming API ì •ìƒ ë™ì‘ í™•ì¸

---

## ìœ„í—˜ ìš”ì†Œ ë° ì™„í™”

| ìœ„í—˜ | ì˜í–¥ | ì™„í™” ë°©ë²• |
|------|------|-----------|
| Rig API ë³€ê²½ | ì»´íŒŒì¼ ì‹¤íŒ¨ | rig-core 0.27 ë²„ì „ ê³ ì • |
| ì²´í¬í¬ì¸íŠ¸ í¬ê¸° | ì„±ëŠ¥ ì €í•˜ | ì••ì¶• ì˜µì…˜ ê¸°ë³¸ í™œì„±í™” |
| Streaming ë³µì¡ì„± | ì¼ì • ì§€ì—° | í•„ìš”ì‹œ fallback ìœ ì§€ |
| ë©”ì‹œì§€ ë³€í™˜ ì˜¤ë¥˜ | LLM ì‘ë‹µ í’ˆì§ˆ ì €í•˜ | ìƒì„¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ |

---

## ë‹¤ìŒ ë‹¨ê³„ (P1)

P0 ì™„ë£Œ í›„:
1. **Streaming ì™„ì „ êµ¬í˜„** (P0ì—ì„œ ì—°ê¸° ì‹œ)
2. ì¶”ê°€ LLM í”„ë¡œë°”ì´ë” ì„¤ì • (Gemini, Ollama, etc.)
3. ë¶„ì‚° ì‹¤í–‰ ì§€ì› (Redis ê¸°ë°˜ ë©”ì‹œì§€ í)
4. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ìµœì í™”
5. Context-length ê´€ë¦¬ (truncation/summarization)

---

## Appendix: Codex Review ì›ë³¸

### Issues ë°œê²¬

1. **[Critical]** `resume()` ë™ì‘ ë¶ˆê°€ - `run_inner`ê°€ í•­ìƒ `superstep = 0`ë¶€í„° ì‹œì‘
2. **[High]** Checkpointê°€ `WorkflowMessage` í•˜ë“œì½”ë”©
3. **[High]** `retry_counts` ë¯¸ì²´í¬í¬ì¸íŒ…
4. **[High]** Rig API ë¶ˆì¼ì¹˜ (`MultiTurnStreamItem` vs `Result<String>`)
5. **[Medium]** Tool call íŒŒì‹±ì´ pure JSONë§Œ ì²˜ë¦¬

### ê¶Œì¥ ê°œì„ ì‚¬í•­

1. Checkpointingì„ `WorkflowMessage` ì „ìš© impl blockìœ¼ë¡œ íŠ¹ìˆ˜í™”
2. `run_from_checkpoint(state, superstep)` API ë„ì…
3. `retry_counts`ë¥¼ ë©”íƒ€ë°ì´í„°ì— í¬í•¨
4. Rig Chat API ì‚¬ìš© (`agent.completion(prompt, history)`)
5. Feature flag ê¸°ë°˜ ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸

### ëˆ„ë½ëœ ê³ ë ¤ì‚¬í•­

- Workflow ID ìƒì„± ë° ê²€ì¦
- ê·¸ë˜í”„ êµ¬ì¡° ë³€ê²½ ì‹œ ë™ì‘
- LLMConfig í•„ë“œ (model/api_base) ì²˜ë¦¬
- Context-length ê´€ë¦¬
- ExecutionModeë³„ resume í…ŒìŠ¤íŠ¸ ë§¤íŠ¸ë¦­ìŠ¤
